{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef94f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "from pathlib import Path\n",
    "import unimib_snowit_project.utils as u\n",
    "import numpy as np\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872f3a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users loaded: 728598 rows, 14 columns\n",
      "profiles loaded: 69025 rows, 10 columns\n",
      "cards loaded: 626523 rows, 5 columns\n",
      "orders loaded: 385168 rows, 13 columns\n",
      "order_details loaded: 993037 rows, 15 columns\n",
      "reviews loaded: 93429 rows, 3 columns\n",
      "reviews_labelled loaded: 327522 rows, 3 columns\n"
     ]
    }
   ],
   "source": [
    "# root directory and pkl folder\n",
    "root_dir_path = u.get_root_dir()\n",
    "df_in_dir = \"data_loaded\"\n",
    "data_pkl_dir_path = root_dir_path.joinpath(df_in_dir)\n",
    "\n",
    "# pickle filenames\n",
    "pkl_files = [\n",
    "    'users.pkl',\n",
    "    'profiles.pkl',\n",
    "    'cards.pkl',\n",
    "    'orders.pkl',\n",
    "    'order_details.pkl',\n",
    "    'reviews.pkl',\n",
    "    'reviews_labelled.pkl'\n",
    "]\n",
    "\n",
    "# dynamic generation of paths\n",
    "pkl_paths = {file_name.split('.')[0]: data_pkl_dir_path.joinpath(file_name) for file_name in pkl_files}\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# loop on files\n",
    "for name, path in pkl_paths.items():\n",
    "    dfs[name] = pd.read_pickle(path)\n",
    "    print(f\"{name} loaded: {dfs[name].shape[0]} rows, {dfs[name].shape[1]} columns\")\n",
    "\n",
    "# Now you can access to data through:\n",
    "users_df = dfs['users']\n",
    "profiles_df = dfs['profiles']\n",
    "cards_df = dfs['cards']\n",
    "orders_df = dfs['orders']   \n",
    "order_details_df = dfs['order_details']\n",
    "reviews_df = dfs['reviews'] \n",
    "labelled_reviews_df = dfs['reviews_labelled']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21895226",
   "metadata": {},
   "source": [
    "# CHECK IF ALL COLUMNS OF ALL FILES ARE CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a964b23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking dataset: users\n",
      "  - Column 'user.uid' seems already clean\n",
      "  - Column 'source' seems already clean\n",
      "  - Column 'city' seems already clean\n",
      "  - Column 'language' seems already clean\n",
      "  - Column 'referral.medium' seems already clean\n",
      "  - Column 'referral.source' seems already clean\n",
      "  - Column 'favouriteZones' contains text that needs cleaning\n",
      "\n",
      "Checking dataset: profiles\n",
      "  - Column 'user.uid' seems already clean\n",
      "  - Column 'profile.uid' seems already clean\n",
      "  - Column 'sex' seems already clean\n",
      "  - Column 'city' seems already clean\n",
      "  - Column 'level' seems already clean\n",
      "  - Column 'types' seems already clean\n",
      "  All text columns seem already clean ✅\n",
      "\n",
      "Checking dataset: cards\n",
      "  - Column 'card.uid' seems already clean\n",
      "  - Column 'status' seems already clean\n",
      "  - Column 'user.uid' seems already clean\n",
      "  All text columns seem already clean ✅\n",
      "\n",
      "Checking dataset: orders\n",
      "  - Column 'order.uid' seems already clean\n",
      "  - Column 'user.uid' seems already clean\n",
      "  - Column 'paymentGateway' seems already clean\n",
      "  - Column 'paymentBrand' seems already clean\n",
      "  - Column 'source' seems already clean\n",
      "  - Column 'tenant' seems already clean\n",
      "  - Column 'timeZone' seems already clean\n",
      "  - Column 'clientInfo' seems already clean\n",
      "  All text columns seem already clean ✅\n",
      "\n",
      "Checking dataset: order_details\n",
      "  - Column 'item.uid' seems already clean\n",
      "  - Column 'order.uid' seems already clean\n",
      "  - Column 'product.uid' seems already clean\n",
      "  - Column 'product.type' seems already clean\n",
      "  - Column 'item.zoneName' contains text that needs cleaning\n",
      "  - Column 'item.profiles' seems already clean\n",
      "  - Column 'item.variantName' contains text that needs cleaning\n",
      "  - Column 'item.slotName' contains text that needs cleaning\n",
      "  - Column 'item.snowitcardNumber' seems already clean\n",
      "  - Column 'item.status' seems already clean\n",
      "\n",
      "Checking dataset: reviews\n",
      "  - Column 'review.uid' seems already clean\n",
      "  - Column 'user.uid' seems already clean\n",
      "  - Column 'text' contains text that needs cleaning\n",
      "\n",
      "Checking dataset: reviews_labelled\n",
      "  - Column 'labelled_review.uid' seems already clean\n",
      "  - Column 'text' contains text that needs cleaning\n",
      "  - Column 'sentiment_label' seems already clean\n"
     ]
    }
   ],
   "source": [
    "# ftfy check function\n",
    "def check_ftfy_needed(df: pd.DataFrame, name: str):\n",
    "    string_cols = df.select_dtypes(include=['object', 'string'])\n",
    "    print(f\"\\nChecking dataset: {name}\")\n",
    "    any_issues = False\n",
    "\n",
    "    for col in string_cols:\n",
    "\n",
    "        def safe_check(x):\n",
    "            try:\n",
    "                if pd.isna(x):\n",
    "                    return False\n",
    "                return str(x) != fix_text(str(x))\n",
    "            except Exception:\n",
    "                # if there is an unhandled value, we consider it clean for safety\n",
    "                return False\n",
    "\n",
    "        diffs = df[col].apply(safe_check)\n",
    "    \n",
    "        if diffs.any():\n",
    "            print(f\"  - Column '{col}' contains text that needs cleaning\")\n",
    "            any_issues = True\n",
    "        else:\n",
    "            print(f\"  - Column '{col}' seems already clean\")\n",
    "    \n",
    "    if not any_issues:\n",
    "        print(\"  All text columns seem already clean ✅\")\n",
    "\n",
    "\n",
    "# loop through all DataFrames already loaded\n",
    "for name, df in dfs.items():\n",
    "    check_ftfy_needed(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e96723",
   "metadata": {},
   "source": [
    "the columns \n",
    "- 'favouriteZones' of users_df,\n",
    "- 'item.zoneName', 'item.variantName', 'item.slotName' of  order_details_df\n",
    "- 'text' of reviews_df,\n",
    "- 'text' of reviews_labelled_df \n",
    "\n",
    "need more adjusments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e361ba",
   "metadata": {},
   "source": [
    "## CHECK WHY COLUMNS NEED ADJUSTMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fb7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean mask: rows where ftfy would change the value\n",
    "def needs_fix(x):\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return False\n",
    "        return str(x) != fix_text(str(x))\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a23d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_value(x):\n",
    "    try:\n",
    "        # Leave NaN or None unchanged\n",
    "        if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "            return x\n",
    "        \n",
    "        # If x is a list, fix each element\n",
    "        if isinstance(x, list):\n",
    "            return [fix_text(str(item)) for item in x]\n",
    "        \n",
    "        # Otherwise, fix the single value\n",
    "        return fix_text(str(x))\n",
    "    except Exception:\n",
    "        # In case of unexpected values, return original\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22026212",
   "metadata": {},
   "source": [
    "### USERS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c786399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows that would need fixing in 'favouriteZones': 54\n",
      "\n",
      "Problematic values (original vs corrected):\n",
      "\n",
      "Original: ('Gressoney-La-Trinit√© - Monterosa Ski',)\n",
      "Corrected: ['Gressoney-La-Trinité - Monterosa Ski']\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Unique rows that needed fixing: 1\n"
     ]
    }
   ],
   "source": [
    "col = 'favouriteZones'  # column to check\n",
    "\n",
    "to_fix = users_df[col].apply(needs_fix)\n",
    "\n",
    "# Count total problematic rows\n",
    "num_to_fix = to_fix.sum()\n",
    "print(f\"Total rows that would need fixing in '{col}': {num_to_fix}\")\n",
    "\n",
    "if num_to_fix > 0:\n",
    "    print(\"\\nProblematic values (original vs corrected):\")\n",
    "\n",
    "    # Take at most 5 rows for examples\n",
    "    problematic_rows = users_df.loc[to_fix, col]\n",
    "    # Convert lists to tuples for uniqueness\n",
    "    unique_values = set()\n",
    "    for val in problematic_rows:\n",
    "        if isinstance(val, list):\n",
    "            unique_values.add(tuple(val))\n",
    "        else:\n",
    "            unique_values.add(val)\n",
    "\n",
    "    # Print unique values\n",
    "    for val in unique_values:\n",
    "        if isinstance(val, tuple):\n",
    "            corrected = [fix_text(str(x)) for x in val]\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {corrected}\")\n",
    "        else:\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {fix_text(str(val))}\")\n",
    "    \n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(f\"\\nUnique rows that needed fixing: {len(unique_values)}\")\n",
    "else:\n",
    "    print(\"All values appear to be clean ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60832d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278       [Alagna - Monterosa Ski, Gressoney-La-Trinité ...\n",
      "347                                                  [Pila]\n",
      "382                                                  [Pila]\n",
      "431                  [Chiesa Valmalenco Bernina Ski Resort]\n",
      "460                                                  [Pila]\n",
      "                                ...                        \n",
      "728232                           [Corno alle Scale, Cimone]\n",
      "728239    [Andalo, Badia - Alta Badia, Cimone, Corno all...\n",
      "728363                                           [Folgaria]\n",
      "728429                                            [Livigno]\n",
      "728493                                            [Foppolo]\n",
      "Name: favouriteZones, Length: 4631, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply the correction directly to the existing column\n",
    "users_df[col] = users_df[col].apply(fix_value)\n",
    "\n",
    "# Quick check of the corrected values\n",
    "print(users_df.loc[users_df['favouriteZones'].apply(lambda x: isinstance(x, list) and len(x) > 0), 'favouriteZones'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426be13",
   "metadata": {},
   "source": [
    "### ORDER DETAILS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb785fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Column: item.zoneName ---\n",
      "Total rows that would need fixing: 2\n",
      "\n",
      "Unique problematic values (original vs corrected):\n",
      "\n",
      "Original: Lago d’Iseo e Franciacorta\n",
      "Corrected: Lago d'Iseo e Franciacorta\n",
      "\n",
      "Unique rows that needed fixing: 1\n",
      "\n",
      "--- Column: item.variantName ---\n",
      "Total rows that would need fixing: 3\n",
      "\n",
      "Unique problematic values (original vs corrected):\n",
      "\n",
      "Original: E-Track 24”\n",
      "Corrected: E-Track 24\"\n",
      "\n",
      "Original: Rossignol E-Track 24”\n",
      "Corrected: Rossignol E-Track 24\"\n",
      "\n",
      "Unique rows that needed fixing: 2\n",
      "\n",
      "--- Column: item.slotName ---\n",
      "Total rows that would need fixing: 1\n",
      "\n",
      "Unique problematic values (original vs corrected):\n",
      "\n",
      "Original: 24”\n",
      "Corrected: 24\"\n",
      "\n",
      "Unique rows that needed fixing: 1\n"
     ]
    }
   ],
   "source": [
    "# Columns to check\n",
    "cols = ['item.zoneName', 'item.variantName', 'item.slotName']\n",
    "\n",
    "for col in cols:\n",
    "    print(f\"\\n--- Column: {col} ---\")\n",
    "\n",
    "    to_fix = order_details_df[col].apply(needs_fix)\n",
    "\n",
    "    # Count total problematic rows\n",
    "    num_to_fix = to_fix.sum()\n",
    "    print(f\"Total rows that would need fixing: {num_to_fix}\")\n",
    "\n",
    "    if num_to_fix == 0:\n",
    "        print(\"All values appear to be clean ✅\")\n",
    "        continue\n",
    "\n",
    "    # Select problematic rows\n",
    "    problematic_rows = order_details_df.loc[to_fix, col]\n",
    "\n",
    "    # Collect unique problematic values\n",
    "    unique_values = set()\n",
    "    for val in problematic_rows:\n",
    "        if isinstance(val, list):\n",
    "            unique_values.add(tuple(val))  # convert lists to tuples for uniqueness\n",
    "        else:\n",
    "            unique_values.add(val)\n",
    "\n",
    "    # Print unique values and their corrected version\n",
    "    print(\"\\nUnique problematic values (original vs corrected):\")\n",
    "    for val in unique_values:\n",
    "        if isinstance(val, tuple):\n",
    "            corrected = [fix_text(str(x)) for x in val]\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {corrected}\")\n",
    "        else:\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {fix_text(str(val))}\")\n",
    "\n",
    "    print(f\"\\nUnique rows that needed fixing: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97eb6b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item.zoneName</th>\n",
       "      <th>item.variantName</th>\n",
       "      <th>item.slotName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bormio</td>\n",
       "      <td>Rossignol Mandate Shift</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Desenzano del Garda</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Desenzano del Garda</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Desenzano del Garda</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Desenzano del Garda</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          item.zoneName         item.variantName item.slotName\n",
       "0                Bormio  Rossignol Mandate Shift             L\n",
       "18  Desenzano del Garda                     <NA>          <NA>\n",
       "19  Desenzano del Garda                     <NA>          <NA>\n",
       "20  Desenzano del Garda                     <NA>          <NA>\n",
       "21  Desenzano del Garda                     <NA>          <NA>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to apply the corrections we use the function previosly defined\n",
    "# Apply the correction directly to each column\n",
    "for col in cols:\n",
    "    order_details_df[col] = order_details_df[col].apply(fix_value)\n",
    "\n",
    "# Quick check\n",
    "order_details_df[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9077e78",
   "metadata": {},
   "source": [
    "### Reviews DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa1ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows that would need fixing in 'text': 103\n",
      "\n",
      "Problematic values (original vs corrected):\n",
      "\n",
      "Original: Recieved quickly. The one pound bag is huge....came in compressed air tight foil bag. Just recently read about how this was the medicinal one to use, \"The amazing healing properties of Manoka honey and cinnamon\".[...]&lt;This is where infor came from. I've had miraculous results with the manuka honey, hope adding this cinnamon adds to it. I got the Manuka honey from Amazon 20%... 15% works just as well from Whole Foods or Amazon. I bought capsules a while back...read the back label & they were the cheaper Cassia cinnamon. You don't want Chinese or Saigon or Cassia...only use Ceylon. One bag of these cinnamon sticks could be shared with a few friends...such a large quantity. I like to grind fresh, or you can get the powdered.\n",
      "Corrected: Recieved quickly. The one pound bag is huge....came in compressed air tight foil bag. Just recently read about how this was the medicinal one to use, \"The amazing healing properties of Manoka honey and cinnamon\".[...]<This is where infor came from. I've had miraculous results with the manuka honey, hope adding this cinnamon adds to it. I got the Manuka honey from Amazon 20%... 15% works just as well from Whole Foods or Amazon. I bought capsules a while back...read the back label & they were the cheaper Cassia cinnamon. You don't want Chinese or Saigon or Cassia...only use Ceylon. One bag of these cinnamon sticks could be shared with a few friends...such a large quantity. I like to grind fresh, or you can get the powdered.\n",
      "\n",
      "Original: Harney &amp; Sons Organic Passion Plum is an excellent way to ease out of caffeine addiction.  Although it won't keep you awake like caffeine, it prevents the headaches that most people encounter with quiting. Be sure to preheat the cup to maximize the ginseng, steep at least five minutes, and above all, enjoy!\n",
      "Corrected: Harney & Sons Organic Passion Plum is an excellent way to ease out of caffeine addiction.  Although it won't keep you awake like caffeine, it prevents the headaches that most people encounter with quiting. Be sure to preheat the cup to maximize the ginseng, steep at least five minutes, and above all, enjoy!\n",
      "\n",
      "Original: The description says 12/5 oz bottles, but you only get one bottle. What a rip off! The sauce is delicious, but you'd be better off buying from the manufacturer ([&#8230;]). Or if you are in Boulder, stop by Vitamin Cottage ([&#8230;]).\n",
      "Corrected: The description says 12/5 oz bottles, but you only get one bottle. What a rip off! The sauce is delicious, but you'd be better off buying from the manufacturer ([…]). Or if you are in Boulder, stop by Vitamin Cottage ([…]).\n",
      "\n",
      "Original: The additional spices, especially the hickory really add something special to this sauce. As with the Jalape&ntilde;o sauce we've  used this on burgers, sandwiches, and Mexican meals at home. As much as we like the Jalape&ntilde;o, this one tops it because of the awesome hickory note balancing the heat of the Chipotle. Two thumbs up and a big hit with our family.\n",
      "Corrected: The additional spices, especially the hickory really add something special to this sauce. As with the Jalapeño sauce we've  used this on burgers, sandwiches, and Mexican meals at home. As much as we like the Jalapeño, this one tops it because of the awesome hickory note balancing the heat of the Chipotle. Two thumbs up and a big hit with our family.\n",
      "\n",
      "Original: I love these gummies like big time, they're the best...but the bag should be a little stronger though!&iexcl;! And it should have a ziplock too!\n",
      "Corrected: I love these gummies like big time, they're the best...but the bag should be a little stronger though!¡! And it should have a ziplock too!\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Unique rows that needed fixing: 103\n"
     ]
    }
   ],
   "source": [
    "col = 'text'  # column to check\n",
    "\n",
    "to_fix = reviews_df[col].apply(needs_fix)\n",
    "\n",
    "# Count total problematic rows\n",
    "num_to_fix = to_fix.sum()\n",
    "print(f\"Total rows that would need fixing in '{col}': {num_to_fix}\")\n",
    "\n",
    "if num_to_fix > 0:\n",
    "    print(\"\\nProblematic values (original vs corrected):\")\n",
    "\n",
    "    # Take at most 5 rows for examples\n",
    "    problematic_rows = reviews_df.loc[to_fix, col]\n",
    "    # Convert lists to tuples for uniqueness\n",
    "    unique_values = set()\n",
    "    for val in problematic_rows:\n",
    "        if isinstance(val, list):\n",
    "            unique_values.add(tuple(val))\n",
    "        else:\n",
    "            unique_values.add(val)\n",
    "   \n",
    "\n",
    "    # Print unique values\n",
    "    for val in list(unique_values)[:5]:\n",
    "        if isinstance(val, tuple):\n",
    "            corrected = [fix_text(str(x)) for x in val]\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {corrected}\")\n",
    "        else:\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {fix_text(str(val))}\")\n",
    "    \n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(f\"\\nUnique rows that needed fixing: {len(unique_values)}\")\n",
    "else:\n",
    "    print(\"All values appear to be clean ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "316601d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have bought several of the Vitality canned d...\n",
       "1    Product arrived labeled as Jumbo Salted Peanut...\n",
       "2    This is a confection that has been around a fe...\n",
       "3    If you are looking for the secret ingredient i...\n",
       "4    Great taffy at a great price.  There was a wid...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to apply the corrections we use the function previously defined\n",
    "\n",
    "# Apply the correction directly to the existing column\n",
    "reviews_df[col] = reviews_df[col].apply(fix_value)\n",
    "\n",
    "# Quick check of the corrected values\n",
    "reviews_df[col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c49aa",
   "metadata": {},
   "source": [
    "### Reviewes Labelled DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90dd860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows that would need fixing in 'text': 428\n",
      "\n",
      "Problematic values (original vs corrected):\n",
      "\n",
      "Original: This is the best strawberry jam ever.  We've tried all the gourmet brands as well as homemade jams from the farmers market but this beats them all by a mile.  It tastes less like sugar and more like ripe strawberries.  The texture is also better because the fruit is pur&eacute;ed so it spreads smoothly.  AND it's organic. :-)\n",
      "Corrected: This is the best strawberry jam ever.  We've tried all the gourmet brands as well as homemade jams from the farmers market but this beats them all by a mile.  It tastes less like sugar and more like ripe strawberries.  The texture is also better because the fruit is puréed so it spreads smoothly.  AND it's organic. :-)\n",
      "\n",
      "Original: El Pato Jalape&ntilde;o Salsa is a great tasting, versatile tomato product.  The sauce can be used straight out of the can for chip-dipping salsa, or it could be used in a variety of recipes.  For example, it could make a great, spicy sauce for baked chicken.  I've used it in soups and also mixed it with sour cream and served it with tortilla chips.\n",
      "Corrected: El Pato Jalapeño Salsa is a great tasting, versatile tomato product.  The sauce can be used straight out of the can for chip-dipping salsa, or it could be used in a variety of recipes.  For example, it could make a great, spicy sauce for baked chicken.  I've used it in soups and also mixed it with sour cream and served it with tortilla chips.\n",
      "\n",
      "Original: Think you're eating real San marzano tomatoes? Think again. The can should have a certificate number and these do. These are the real McCoy. And they cook up deliicious. What I like to do is to saut&eacute; some onion and garlic, a little red pepper flake, add my tomatoes that I've pureed with a hand blender (or use a food mill, if you prefer), then let simmer for about 20 minutes until thick and creamy. Nothing's better for homemade ravioli or freshly made pizza, or my personal favorite, pasta bolognese. If shipping seems out of whack, just order more cans...that should offset the cost immeasurably. Enjoy!!!\n",
      "Corrected: Think you're eating real San marzano tomatoes? Think again. The can should have a certificate number and these do. These are the real McCoy. And they cook up deliicious. What I like to do is to sauté some onion and garlic, a little red pepper flake, add my tomatoes that I've pureed with a hand blender (or use a food mill, if you prefer), then let simmer for about 20 minutes until thick and creamy. Nothing's better for homemade ravioli or freshly made pizza, or my personal favorite, pasta bolognese. If shipping seems out of whack, just order more cans...that should offset the cost immeasurably. Enjoy!!!\n",
      "\n",
      "Original: This is another good Zatarain's Cajun base. The grocers in my area don't offer this one, SO... The &Eacute;touff&eacute;e base and the creole base are my favorites. The price with shipping is reasonable. I like to keep a stock on hand. We enjoy a lot of seafood and other Cajun foods.\n",
      "Corrected: This is another good Zatarain's Cajun base. The grocers in my area don't offer this one, SO... The Étouffée base and the creole base are my favorites. The price with shipping is reasonable. I like to keep a stock on hand. We enjoy a lot of seafood and other Cajun foods.\n",
      "\n",
      "Original: I purchased this plant Jan 4 and it arrived like 5 days later.Its an amazing specimen it came nicely and securely packed,kudos to the seller! All intact except for a little dent on one of the leaves, tip was folded which did not really affect the health of the plant.I'm surprised it has like 3 growing plants in the 4\"pot, so I have like 3 plants in one pot.I noticed it has roots in every segment,meaning its easy to propagate.For now it is happily hanging in my bathroom since its too cold to bring it out.I plan to plant one  outside on a trellis next to all my other orchids hoping it grows and mature into a huge plant.Then again I have 2 more that can happily stay potted.I got a really good deal for the price.Thanks Amazon &lt;3\n",
      "Corrected: I purchased this plant Jan 4 and it arrived like 5 days later.Its an amazing specimen it came nicely and securely packed,kudos to the seller! All intact except for a little dent on one of the leaves, tip was folded which did not really affect the health of the plant.I'm surprised it has like 3 growing plants in the 4\"pot, so I have like 3 plants in one pot.I noticed it has roots in every segment,meaning its easy to propagate.For now it is happily hanging in my bathroom since its too cold to bring it out.I plan to plant one  outside on a trellis next to all my other orchids hoping it grows and mature into a huge plant.Then again I have 2 more that can happily stay potted.I got a really good deal for the price.Thanks Amazon <3\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Unique rows that needed fixing: 428\n"
     ]
    }
   ],
   "source": [
    "col = 'text'  # column to check\n",
    "\n",
    "to_fix = labelled_reviews_df[col].apply(needs_fix)\n",
    "\n",
    "# Count total problematic rows\n",
    "num_to_fix = to_fix.sum()\n",
    "print(f\"Total rows that would need fixing in '{col}': {num_to_fix}\")\n",
    "\n",
    "if num_to_fix > 0:\n",
    "    print(\"\\nProblematic values (original vs corrected):\")\n",
    "\n",
    "    # Take at most 5 rows for examples\n",
    "    problematic_rows = labelled_reviews_df.loc[to_fix, col]\n",
    "    # Convert lists to tuples for uniqueness\n",
    "    unique_values = set()\n",
    "    for val in problematic_rows:\n",
    "        if isinstance(val, list):\n",
    "            unique_values.add(tuple(val))\n",
    "        else:\n",
    "            unique_values.add(val)\n",
    "   \n",
    "\n",
    "    # Print unique values\n",
    "    for val in list(unique_values)[:5]:\n",
    "        if isinstance(val, tuple):\n",
    "            corrected = [fix_text(str(x)) for x in val]\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {corrected}\")\n",
    "        else:\n",
    "            print(f\"\\nOriginal: {val}\")\n",
    "            print(f\"Corrected: {fix_text(str(val))}\")\n",
    "    \n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(f\"\\nUnique rows that needed fixing: {len(unique_values)}\")\n",
    "else:\n",
    "    print(\"All values appear to be clean ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e79df006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I'm no bitters expert but I bought it as a gif...\n",
       "1    these are probably great in the right drinks, ...\n",
       "2    I sent these to my dad for his bday and he sai...\n",
       "3    I purchased these as a gift for family member ...\n",
       "4    My wife bought me this sauce sampler for Chris...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to apply the corrections we use the function previosly defined\n",
    "\n",
    "# Apply the correction directly to the existing column\n",
    "labelled_reviews_df[col] = labelled_reviews_df[col].apply(fix_value)\n",
    "\n",
    "# Quick check of the corrected values\n",
    "labelled_reviews_df[col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b5665",
   "metadata": {},
   "source": [
    "# HANDLING MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "524e865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_info(df, name):\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(df.info())\n",
    "    print(\"Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce431746",
   "metadata": {},
   "source": [
    "## USERS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b35b0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Users =====\n",
      "Shape: (728598, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 728598 entries, 0 to 728597\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   user.uid         728598 non-null  string        \n",
      " 1   createdAt        715839 non-null  datetime64[ns]\n",
      " 2   source           728598 non-null  string        \n",
      " 3   isAnonymous      659772 non-null  boolean       \n",
      " 4   referralsCount   177554 non-null  Int64         \n",
      " 5   city             329391 non-null  object        \n",
      " 6   language         493253 non-null  string        \n",
      " 7   googleId         153914 non-null  boolean       \n",
      " 8   appleId          11647 non-null   boolean       \n",
      " 9   facebookId       62775 non-null   boolean       \n",
      " 10  referral.medium  22474 non-null   object        \n",
      " 11  referral.source  23199 non-null   object        \n",
      " 12  referral.type    23117 non-null   Int64         \n",
      " 13  favouriteZones   728598 non-null  object        \n",
      "dtypes: Int64(2), boolean(4), datetime64[ns](1), object(4), string(3)\n",
      "memory usage: 62.5+ MB\n",
      "None\n",
      "Missing values per column:\n",
      "user.uid                0\n",
      "createdAt           12759\n",
      "source                  0\n",
      "isAnonymous         68826\n",
      "referralsCount     551044\n",
      "city               399207\n",
      "language           235345\n",
      "googleId           574684\n",
      "appleId            716951\n",
      "facebookId         665823\n",
      "referral.medium    706124\n",
      "referral.source    705399\n",
      "referral.type      705481\n",
      "favouriteZones          0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "basic_info(users_df, \"Users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d3142e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "appleId → 2 unique values\n",
      "<BooleanArray>\n",
      "[<NA>, True]\n",
      "Length: 2, dtype: boolean\n",
      "\n",
      "googleId → 2 unique values\n",
      "<BooleanArray>\n",
      "[True, <NA>]\n",
      "Length: 2, dtype: boolean\n",
      "\n",
      "facebookId → 2 unique values\n",
      "<BooleanArray>\n",
      "[<NA>, True]\n",
      "Length: 2, dtype: boolean\n"
     ]
    }
   ],
   "source": [
    "for col in ['appleId','googleId','facebookId']:\n",
    "    uniques = users_df[col].unique()\n",
    "    print(f\"\\n{col} → {len(uniques)} unique values\")\n",
    "    print(uniques) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef929f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appleId → [False  True]\n",
      "googleId → [ True False]\n",
      "facebookId → [False  True]\n"
     ]
    }
   ],
   "source": [
    "# false values are interpeted as missing, so I change them\n",
    "\n",
    "for col in ['appleId', 'googleId', 'facebookId']:\n",
    "    users_df[col] = users_df[col].fillna(False).astype(bool)\n",
    "    uniques = users_df[col].unique()\n",
    "    print(f\"{col} → {uniques}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f06770d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid                0\n",
       "createdAt           12759\n",
       "source                  0\n",
       "isAnonymous         68826\n",
       "referralsCount     551044\n",
       "city               399207\n",
       "language           235345\n",
       "googleId                0\n",
       "appleId                 0\n",
       "facebookId              0\n",
       "referral.medium    706124\n",
       "referral.source    705399\n",
       "referral.type      705481\n",
       "favouriteZones          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95c67479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid                0\n",
       "createdAt           10371\n",
       "source                  0\n",
       "isAnonymous         68764\n",
       "referralsCount     551044\n",
       "city               328790\n",
       "language           194879\n",
       "googleId                0\n",
       "appleId                 0\n",
       "facebookId              0\n",
       "referral.medium    533606\n",
       "referral.source    532842\n",
       "referral.type      533051\n",
       "favouriteZones          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df[users_df['referralsCount'].isna()].isnull().sum()\n",
    "# the majority of the users with missing referralsCount have also missing values in referral.medium, referral.source, referral.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5774332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set missing referralsCount to 0\n",
    "users_df['referralsCount'] = users_df['referralsCount'].fillna(0).astype(int)\n",
    "\n",
    "# Set missing referral.medium and referral.source to \"none\"\n",
    "for col in ['referral.medium','referral.source']:\n",
    "    users_df[col] = users_df[col].fillna(\"none\")\n",
    "\n",
    "# Set missing referral.type to -1  \n",
    "users_df['referral.type'] = users_df['referral.type'].fillna(-1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f573a50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BooleanArray>\n",
       "[False, True, <NA>]\n",
       "Length: 3, dtype: boolean"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df['isAnonymous'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c12769be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68826, 14)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df[users_df['isAnonymous'].isna()].shape\n",
    "# 68826 null over 728598 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29fc29a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39046, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = users_df[\n",
    "    (users_df[\"appleId\"] == False) &\n",
    "    (users_df[\"googleId\"] == False) &\n",
    "    (users_df[\"facebookId\"] == False) &\n",
    "    (users_df[\"isAnonymous\"].isna())\n",
    "]\n",
    "\n",
    "filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad99a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set missing isAnonymous to True when the users didn't register neither via Apple nor via Google nor via Facebook\n",
    "\n",
    "users_df.loc[\n",
    "    (users_df[\"appleId\"] == False) &\n",
    "    (users_df[\"googleId\"] == False) &\n",
    "    (users_df[\"facebookId\"] == False) &\n",
    "    (users_df[\"isAnonymous\"].isna()),\n",
    "    \"isAnonymous\"\n",
    "] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a3ef764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set resting missing isAnonymous to False\n",
    "users_df['isAnonymous'] = users_df['isAnonymous'].fillna(False).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38419214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid                0\n",
       "createdAt           12759\n",
       "source                  0\n",
       "isAnonymous             0\n",
       "referralsCount          0\n",
       "city               399207\n",
       "language           235345\n",
       "googleId                0\n",
       "appleId                 0\n",
       "facebookId              0\n",
       "referral.medium         0\n",
       "referral.source         0\n",
       "referral.type           0\n",
       "favouriteZones          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5292144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 'language isn't handled since the main information can be retrieved from the 'city' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05c9c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set missing city and language to \"none\"\n",
    "for col in ['city','language']:\n",
    "    users_df[col] = users_df[col].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7c13140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid               0\n",
       "createdAt          12759\n",
       "source                 0\n",
       "isAnonymous            0\n",
       "referralsCount         0\n",
       "city                   0\n",
       "language               0\n",
       "googleId               0\n",
       "appleId                0\n",
       "facebookId             0\n",
       "referral.medium        0\n",
       "referral.source        0\n",
       "referral.type          0\n",
       "favouriteZones         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.isnull().sum() \n",
    "\n",
    "# since the 'createdAt' feature is essential for the following analysis related to the RFM and Churn models, it is temporally kept with some null values that\n",
    "# will be carefully handled when necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1fe27",
   "metadata": {},
   "source": [
    "## PROFILES DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16878ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Profiles =====\n",
      "Shape: (69025, 10)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 69025 entries, 1 to 97157\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   user.uid      69025 non-null  string        \n",
      " 1   profile.uid   69025 non-null  string        \n",
      " 2   birthday      64521 non-null  datetime64[ns]\n",
      " 3   sex           49069 non-null  object        \n",
      " 4   city          8932 non-null   object        \n",
      " 5   height        53139 non-null  Float64       \n",
      " 6   weight        52958 non-null  float64       \n",
      " 7   skibootsSize  52937 non-null  Float64       \n",
      " 8   level         54138 non-null  string        \n",
      " 9   types         69025 non-null  object        \n",
      "dtypes: Float64(2), datetime64[ns](1), float64(1), object(3), string(3)\n",
      "memory usage: 5.9+ MB\n",
      "None\n",
      "Missing values per column:\n",
      "user.uid            0\n",
      "profile.uid         0\n",
      "birthday         4504\n",
      "sex             19956\n",
      "city            60093\n",
      "height          15886\n",
      "weight          16067\n",
      "skibootsSize    16088\n",
      "level           14887\n",
      "types               0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "basic_info(profiles_df, \"Profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb040a06",
   "metadata": {},
   "source": [
    "As expected some user.uid are associated with more profiles.uid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36ae5b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user.uid with more than one profile: 14494\n"
     ]
    }
   ],
   "source": [
    "# Evaluate how many profile.uid for each user.uid\n",
    "profiles_per_user = profiles_df.groupby(\"user.uid\")[\"profile.uid\"].nunique().reset_index()\n",
    "\n",
    "# Filter only those with more than 1 profile\n",
    "multi_profiles = profiles_per_user[profiles_per_user[\"profile.uid\"] > 1]\n",
    "multi_profiles.columns = [\"user.uid\", \"Number of profiles\"]\n",
    "\n",
    "print(f\"Number of user.uid with more than one profile: {multi_profiles.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa423c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.uid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of profiles</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    user.uid\n",
       "Number of profiles          \n",
       "2                       9732\n",
       "3                       2804\n",
       "4                       1263\n",
       "5                        398\n",
       "6                        132\n",
       "7                         58\n",
       "8                         33\n",
       "9                         18\n",
       "10                         7\n",
       "11                        13\n",
       "12                         7\n",
       "13                         6\n",
       "14                         4\n",
       "15                         3\n",
       "16                         3\n",
       "18                         2\n",
       "19                         1\n",
       "20                         1\n",
       "21                         1\n",
       "26                         1\n",
       "28                         1\n",
       "30                         1\n",
       "31                         1\n",
       "32                         1\n",
       "35                         1\n",
       "68                         1\n",
       "81                         1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_profiles.groupby(\"Number of profiles\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a42fb",
   "metadata": {},
   "source": [
    "We have a lot of users with 2-10 profiles, a few with 11-18 profiles and some users that have created from 19 to 81 different profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37f30c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid            0\n",
       "profile.uid         0\n",
       "birthday         4504\n",
       "sex             19956\n",
       "city            60093\n",
       "height          15886\n",
       "weight          16067\n",
       "skibootsSize    16088\n",
       "level           14887\n",
       "types               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_df.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246d922",
   "metadata": {},
   "source": [
    "we have 60093 missing values in the 'city' column, so we can consider imputing them by using the 'city' column of the users dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0934984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching cities: 4829\n",
      "Mismatched cities: 64196\n",
      "Mismatching cities because of missing values in profiles: 60093\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(\n",
    "    users_df[['user.uid', 'city']], \n",
    "    profiles_df[['user.uid', 'city']], \n",
    "    on='user.uid', \n",
    "    how='inner',  # only consider users present in both datasets\n",
    "    suffixes=('_users', '_profiles')\n",
    ")\n",
    "\n",
    "# create a column to check if the cities match\n",
    "merged_df['city_match'] = merged_df['city_users'] == merged_df['city_profiles']\n",
    "\n",
    "\n",
    "num_match = merged_df['city_match'].sum()\n",
    "num_mismatch = (~merged_df['city_match']).sum()\n",
    "\n",
    "print(f\"Matching cities: {num_match}\")\n",
    "print(f\"Mismatched cities: {num_mismatch}\")\n",
    "\n",
    "\n",
    "print(f\"Mismatching cities because of missing values in profiles: {merged_df['city_profiles'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "296d4000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.uid</th>\n",
       "      <th>city_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09gqlmpl9bn82gwyct3aetbcdp</td>\n",
       "      <td>aachen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lvfkrnkmzixk7nnnuns78po6ku</td>\n",
       "      <td>aarschot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lvfkrnkmzixk7nnnuns78po6ku</td>\n",
       "      <td>aarschot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wmt7ktgwiwhb8qswewseu76e0r</td>\n",
       "      <td>abano terme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>j2bmupwsjofrlqcmlrwdfmvrqh</td>\n",
       "      <td>abano terme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69020</th>\n",
       "      <td>cup5j32rq5fryxrrc1qns4lsyd</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69021</th>\n",
       "      <td>cxrgfedtvwhnshcejppspovzxe</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69022</th>\n",
       "      <td>cxrgfedtvwhnshcejppspovzxe</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69023</th>\n",
       "      <td>cxrgfedtvwhnshcejppspovzxe</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69024</th>\n",
       "      <td>cxrgfedtvwhnshcejppspovzxe</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60093 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         user.uid   city_users\n",
       "0      09gqlmpl9bn82gwyct3aetbcdp       aachen\n",
       "2      lvfkrnkmzixk7nnnuns78po6ku     aarschot\n",
       "3      lvfkrnkmzixk7nnnuns78po6ku     aarschot\n",
       "4      wmt7ktgwiwhb8qswewseu76e0r  abano terme\n",
       "5      j2bmupwsjofrlqcmlrwdfmvrqh  abano terme\n",
       "...                           ...          ...\n",
       "69020  cup5j32rq5fryxrrc1qns4lsyd         none\n",
       "69021  cxrgfedtvwhnshcejppspovzxe         none\n",
       "69022  cxrgfedtvwhnshcejppspovzxe         none\n",
       "69023  cxrgfedtvwhnshcejppspovzxe         none\n",
       "69024  cxrgfedtvwhnshcejppspovzxe         none\n",
       "\n",
       "[60093 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_without_city = merged_df[(merged_df['city_match']==False) & (merged_df['city_profiles'].isnull())][['user.uid', 'city_users']]\n",
    "user_without_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4433de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        user.uid         city\n",
      "4301  009jfacn0eo5nynrv5p3tt2s7h  montelupone\n",
      "4302  00j7rrt6ebzgqzu7epdl2pgmfo      firenze\n",
      "4303  00koqkoxhhwx4egpnlugfmz63h       milano\n",
      "4304  00p4czyqrge7lsrac6npbsqfap         lodi\n",
      "4305  00p4czyqrge7lsrac6npbsqfap         lodi\n"
     ]
    }
   ],
   "source": [
    "# create mapping user.uid -> city_users\n",
    "city_map = dict(zip(user_without_city[\"user.uid\"], user_without_city[\"city_users\"]))\n",
    "\n",
    "\n",
    "mask = (profiles_df[\"user.uid\"].isin(city_map.keys())) & (profiles_df[\"city\"].isnull())\n",
    "profiles_df.loc[mask, \"city\"] = profiles_df.loc[mask, \"user.uid\"].map(city_map)\n",
    "\n",
    "print(profiles_df.loc[mask, [\"user.uid\", \"city\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5ee9c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid            0\n",
       "profile.uid         0\n",
       "birthday         4504\n",
       "sex             19956\n",
       "city                0\n",
       "height          15886\n",
       "weight          16067\n",
       "skibootsSize    16088\n",
       "level           14887\n",
       "types               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac13b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an idea to deal with height, weight and skibootSize could have been to consider the median given the sex and the birthday, but the following table shows that the rows with missing\n",
    "# height, weight and skibootsSize AND sex, are essentialy the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "248f105d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14958, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_df[\n",
    "    (profiles_df[\"height\"].isna()) &\n",
    "    (profiles_df[\"weight\"].isna()) &\n",
    "    (profiles_df[\"skibootsSize\"].isna()) &\n",
    "    (profiles_df[\"sex\"].isna())\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96912476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more standard imputation follows then:\n",
    "\n",
    "for col in ['height','weight', 'skibootsSize']:\n",
    "    profiles_df[col] = profiles_df[col].fillna(-1)\n",
    "\n",
    "for col in ['sex','level']:\n",
    "    profiles_df[col] = profiles_df[col].fillna(\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9d1e3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid           0\n",
       "profile.uid        0\n",
       "birthday        4504\n",
       "sex                0\n",
       "city               0\n",
       "height             0\n",
       "weight             0\n",
       "skibootsSize       0\n",
       "level              0\n",
       "types              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c729dd",
   "metadata": {},
   "source": [
    "## CARDS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab523565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Cards =====\n",
      "Shape: (626523, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 626523 entries, 1 to 805836\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   card.uid    626523 non-null  string        \n",
      " 1   assignedAt  183607 non-null  datetime64[ns]\n",
      " 2   birthday    166055 non-null  datetime64[ns]\n",
      " 3   status      626523 non-null  string        \n",
      " 4   user.uid    184781 non-null  string        \n",
      "dtypes: datetime64[ns](2), string(3)\n",
      "memory usage: 28.7 MB\n",
      "None\n",
      "Missing values per column:\n",
      "card.uid           0\n",
      "assignedAt    442916\n",
      "birthday      460468\n",
      "status             0\n",
      "user.uid      441742\n",
      "dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "basic_info(cards_df, \"Cards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced63464",
   "metadata": {},
   "source": [
    "Remove rows with null user.uid since they would be useless for analysis (can't link the order to any user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15e5b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df = cards_df[cards_df['user.uid'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ef0fa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card.uid          0\n",
       "assignedAt     1992\n",
       "birthday      18907\n",
       "status            0\n",
       "user.uid          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ebabf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as below, missing data would be handled when strictly necessary due to the importance of the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f437a7",
   "metadata": {},
   "source": [
    "## ORDERS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "615761b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Orders =====\n",
      "Shape: (385168, 13)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 385168 entries, 0 to 549899\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   order.uid        385168 non-null  string        \n",
      " 1   user.uid         385168 non-null  string        \n",
      " 2   createdAt        385168 non-null  datetime64[ns]\n",
      " 3   createdAtTime    385168 non-null  datetime64[ns]\n",
      " 4   paymentGateway   385168 non-null  string        \n",
      " 5   paymentBrand     251908 non-null  string        \n",
      " 6   pickup           385168 non-null  boolean       \n",
      " 7   pickupComplete   385168 non-null  boolean       \n",
      " 8   source           385000 non-null  string        \n",
      " 9   tenant           384732 non-null  string        \n",
      " 10  paymentAttempts  385168 non-null  Int64         \n",
      " 11  timeZone         114872 non-null  string        \n",
      " 12  clientInfo       385168 non-null  string        \n",
      "dtypes: Int64(1), boolean(2), datetime64[ns](2), string(8)\n",
      "memory usage: 37.1 MB\n",
      "None\n",
      "Missing values per column:\n",
      "order.uid               0\n",
      "user.uid                0\n",
      "createdAt               0\n",
      "createdAtTime           0\n",
      "paymentGateway          0\n",
      "paymentBrand       133260\n",
      "pickup                  0\n",
      "pickupComplete          0\n",
      "source                168\n",
      "tenant                436\n",
      "paymentAttempts         0\n",
      "timeZone           270296\n",
      "clientInfo              0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "basic_info(orders_df, \"Orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1085d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['paymentBrand','source','tenant']:\n",
    "    orders_df[col] = orders_df[col].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "612ad4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "col='timeZone'\n",
    "orders_df[col] = orders_df[col].fillna(\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f512ce63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order.uid          0\n",
       "user.uid           0\n",
       "createdAt          0\n",
       "createdAtTime      0\n",
       "paymentGateway     0\n",
       "paymentBrand       0\n",
       "pickup             0\n",
       "pickupComplete     0\n",
       "source             0\n",
       "tenant             0\n",
       "paymentAttempts    0\n",
       "timeZone           0\n",
       "clientInfo         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86938c",
   "metadata": {},
   "source": [
    "## ORDERS DETAILS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abe328de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Order Details =====\n",
      "Shape: (993037, 15)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 993037 entries, 0 to 1420602\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   item.uid                993037 non-null  string        \n",
      " 1   order.uid               993037 non-null  string        \n",
      " 2   item.date               956773 non-null  datetime64[ns]\n",
      " 3   product.uid             993037 non-null  string        \n",
      " 4   product.dynamicPricing  993037 non-null  boolean       \n",
      " 5   item.amount             993037 non-null  Float32       \n",
      " 6   item.discount           993037 non-null  boolean       \n",
      " 7   product.type            993037 non-null  string        \n",
      " 8   item.zoneName           993037 non-null  object        \n",
      " 9   product.durationHours   838534 non-null  Float32       \n",
      " 10  item.profiles           36440 non-null   string        \n",
      " 11  item.variantName        993037 non-null  object        \n",
      " 12  item.slotName           993037 non-null  object        \n",
      " 13  item.snowitcardNumber   288271 non-null  string        \n",
      " 14  item.status             993037 non-null  string        \n",
      "dtypes: Float32(2), boolean(2), datetime64[ns](1), object(3), string(7)\n",
      "memory usage: 104.2+ MB\n",
      "None\n",
      "Missing values per column:\n",
      "item.uid                       0\n",
      "order.uid                      0\n",
      "item.date                  36264\n",
      "product.uid                    0\n",
      "product.dynamicPricing         0\n",
      "item.amount                    0\n",
      "item.discount                  0\n",
      "product.type                   0\n",
      "item.zoneName                  0\n",
      "product.durationHours     154503\n",
      "item.profiles             956597\n",
      "item.variantName               0\n",
      "item.slotName                  0\n",
      "item.snowitcardNumber     704766\n",
      "item.status                    0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "basic_info(order_details_df, \"Order Details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5c1d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to impute item.profiles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f71cdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user.uid\n",
       "00c2zutvhrg2wsots6dqti4acu                          [2u6fgglofytfkorfqwjr]\n",
       "00g2ppqciodr2kux1j44mnp9a3                          [cet18sb27ncebvu7kvyl]\n",
       "00j7rrt6ebzgqzu7epdl2pgmfo                          [cuig5qrqxafdgd3g6azy]\n",
       "00koqkoxhhwx4egpnlugfmz63h                          [ddjxqn8bghajd9me60nl]\n",
       "00mr3b8fzvthxearlfpaamtmtr                          [v7tyzgvhlbafsfzbd8mv]\n",
       "                                                  ...                     \n",
       "zztkbfxq5sa5ofpun0df5jddgc                          [cgyskv7mks2xggnx9ivk]\n",
       "zztsqurlqbakodjb55nn3urtlm                          [pa38lutrucpujlq96cfx]\n",
       "zzvnki0kv1vlaq4tztvbthtvfc                          [hqqnb0xi1o1uaktd34cu]\n",
       "zzxaa4iuo4fnkdl2q13q71iu6g                          [far2podl7bhin7u4tds2]\n",
       "zzxhbqd23nyekppw9fvbu5y1sn    [7ul1yl0vozkj4odjftwx, ox9cqfkgsnn4bvo5kndk]\n",
       "Name: profile.uid, Length: 32314, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge order_details with orders\n",
    "details_orders = pd.merge(\n",
    "    order_details_df,\n",
    "    orders_df[[\"order.uid\", \"user.uid\"]], \n",
    "    on=\"order.uid\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Merge with profiles on user.uid\n",
    "details_orders_profiles = pd.merge(\n",
    "    details_orders,\n",
    "    profiles_df[[\"user.uid\", \"profile.uid\"]],\n",
    "    on=\"user.uid\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Extract, for each user.uid, the list of associated profile.uid\n",
    "user_profiles = (\n",
    "    details_orders_profiles.groupby(\"user.uid\")[\"profile.uid\"]\n",
    "    .unique()   \n",
    ")\n",
    "\n",
    "user_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abb8c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since a user can be related to more profiles, and the order is only related with the user, I can't have certainty about the profiles related to that specific order\n",
    "order_details_df['item.profiles'] = order_details_df['item.profiles'].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae32a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving to item.snowitcardNumber..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b69a38a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.uid</th>\n",
       "      <th>card.uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001vktlc5zbkx2bazqkwmrpru9</td>\n",
       "      <td>[02138143]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005dpt4a0puxgicccu6ixvlc9f</td>\n",
       "      <td>[01161471599232720540808]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00bua0ypencroqp5mgcohofq2z</td>\n",
       "      <td>[07754863]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00dija3tzdwkjxszdmgnqjtd8t</td>\n",
       "      <td>[2417458]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00elzcypz9btvvtxxpu74qu6qc</td>\n",
       "      <td>[01161471599233125552422]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81805</th>\n",
       "      <td>zzuz1bgyk6w7mcoknhxb2gj2kl</td>\n",
       "      <td>[08822257]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81807</th>\n",
       "      <td>zzv4z2ddfmeahyytarujh8fn4v</td>\n",
       "      <td>[02022789]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81811</th>\n",
       "      <td>zzwaicicjdwjxtqrln1f4eougn</td>\n",
       "      <td>[03563380]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81813</th>\n",
       "      <td>zzwyguvjxtzadqq0fcswvxiwpk</td>\n",
       "      <td>[03241961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81815</th>\n",
       "      <td>zzy20yyj86wru5mw92znoifwmk</td>\n",
       "      <td>[04254579]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47931 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         user.uid                   card.uid\n",
       "0      001vktlc5zbkx2bazqkwmrpru9                 [02138143]\n",
       "2      005dpt4a0puxgicccu6ixvlc9f  [01161471599232720540808]\n",
       "3      00bua0ypencroqp5mgcohofq2z                 [07754863]\n",
       "6      00dija3tzdwkjxszdmgnqjtd8t                  [2417458]\n",
       "8      00elzcypz9btvvtxxpu74qu6qc  [01161471599233125552422]\n",
       "...                           ...                        ...\n",
       "81805  zzuz1bgyk6w7mcoknhxb2gj2kl                 [08822257]\n",
       "81807  zzv4z2ddfmeahyytarujh8fn4v                 [02022789]\n",
       "81811  zzwaicicjdwjxtqrln1f4eougn                 [03563380]\n",
       "81813  zzwyguvjxtzadqq0fcswvxiwpk                 [03241961]\n",
       "81815  zzy20yyj86wru5mw92znoifwmk                 [04254579]\n",
       "\n",
       "[47931 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge order_details with orders\n",
    "details_orders = pd.merge(\n",
    "    order_details_df,\n",
    "    orders_df[[\"order.uid\", \"user.uid\"]],  \n",
    "    on=\"order.uid\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Merge with cards on user.uid\n",
    "details_orders_cards = pd.merge(\n",
    "    details_orders,\n",
    "    cards_df[[\"user.uid\", \"card.uid\"]],\n",
    "    on=\"user.uid\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Extract, for each user.uid, the list of associated card.uid\n",
    "user_cards = (\n",
    "    details_orders_cards.groupby(\"user.uid\")[\"card.uid\"]\n",
    "    .unique()   \n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Find users with just ONE skipass\n",
    "single_card_users = (\n",
    "    user_cards[user_cards[\"card.uid\"].str.len() == 1] \n",
    ")\n",
    "single_card_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a982d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, I recall the skipass number through a double merge, only if the list of skipass associated with a user is\n",
    "# made by a single skipass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07c3d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order.uid</th>\n",
       "      <th>product.type</th>\n",
       "      <th>item.snowitcardNumber</th>\n",
       "      <th>user.uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4y9zqqvldfqr9n2xnu</td>\n",
       "      <td>rental~bike</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nu7fhz41rzwbkabapcufr6g18r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i9eovtgp3uxbmesebv</td>\n",
       "      <td>experience</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>hkxsbgefntzwsodvj3ztvp3gm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i9eovtgp3uxbmesebv</td>\n",
       "      <td>bundle~train</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>hkxsbgefntzwsodvj3ztvp3gm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i9eovtgp3uxbmesebv</td>\n",
       "      <td>transport</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>hkxsbgefntzwsodvj3ztvp3gm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i9eovtgp3uxbmesebv</td>\n",
       "      <td>service</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>hkxsbgefntzwsodvj3ztvp3gm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993032</th>\n",
       "      <td>f8zj28hdatvraibekk</td>\n",
       "      <td>experience</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>asyeuoax8uaw3np3tlyxpcfxzm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993033</th>\n",
       "      <td>f8zj28hdatvraibekk</td>\n",
       "      <td>experience</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>asyeuoax8uaw3np3tlyxpcfxzm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993034</th>\n",
       "      <td>f8zj28hdatvraibekk</td>\n",
       "      <td>experience</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>asyeuoax8uaw3np3tlyxpcfxzm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993035</th>\n",
       "      <td>wy69h8jsgozdrxlu9i</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>wfsihoehobxkipt3731virtjqq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993036</th>\n",
       "      <td>wy69h8jsgozdrxlu9i</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>wfsihoehobxkipt3731virtjqq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993037 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 order.uid  product.type item.snowitcardNumber  \\\n",
       "0       4y9zqqvldfqr9n2xnu   rental~bike                  <NA>   \n",
       "1       i9eovtgp3uxbmesebv    experience                  <NA>   \n",
       "2       i9eovtgp3uxbmesebv  bundle~train                  <NA>   \n",
       "3       i9eovtgp3uxbmesebv     transport                  <NA>   \n",
       "4       i9eovtgp3uxbmesebv       service                  <NA>   \n",
       "...                    ...           ...                   ...   \n",
       "993032  f8zj28hdatvraibekk    experience                  <NA>   \n",
       "993033  f8zj28hdatvraibekk    experience                  <NA>   \n",
       "993034  f8zj28hdatvraibekk    experience                  <NA>   \n",
       "993035  wy69h8jsgozdrxlu9i       skipass                  <NA>   \n",
       "993036  wy69h8jsgozdrxlu9i       skipass                  <NA>   \n",
       "\n",
       "                          user.uid  \n",
       "0       nu7fhz41rzwbkabapcufr6g18r  \n",
       "1       hkxsbgefntzwsodvj3ztvp3gm1  \n",
       "2       hkxsbgefntzwsodvj3ztvp3gm1  \n",
       "3       hkxsbgefntzwsodvj3ztvp3gm1  \n",
       "4       hkxsbgefntzwsodvj3ztvp3gm1  \n",
       "...                            ...  \n",
       "993032  asyeuoax8uaw3np3tlyxpcfxzm  \n",
       "993033  asyeuoax8uaw3np3tlyxpcfxzm  \n",
       "993034  asyeuoax8uaw3np3tlyxpcfxzm  \n",
       "993035  wfsihoehobxkipt3731virtjqq  \n",
       "993036  wfsihoehobxkipt3731virtjqq  \n",
       "\n",
       "[993037 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge order_details with orders\n",
    "details_orders = pd.merge(\n",
    "    order_details_df[[\"order.uid\", \"product.type\", \"item.snowitcardNumber\"]],\n",
    "    orders_df[[\"order.uid\", \"user.uid\"]],\n",
    "    on=\"order.uid\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "details_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a769073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order.uid</th>\n",
       "      <th>product.type</th>\n",
       "      <th>item.snowitcardNumber</th>\n",
       "      <th>user.uid</th>\n",
       "      <th>card.uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>berfovxwyu72mzwmei</td>\n",
       "      <td>giftcard</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>angticjoeipyve4gpwgwd7nv53</td>\n",
       "      <td>[07240571]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sgo1b64fhatmepxi1z</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>kllr731wlmbb8njvb1v1bavris</td>\n",
       "      <td>[06758428]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sgo1b64fhatmepxi1z</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>kllr731wlmbb8njvb1v1bavris</td>\n",
       "      <td>[06758428]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sgo1b64fhatmepxi1z</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>kllr731wlmbb8njvb1v1bavris</td>\n",
       "      <td>[06758428]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sgo1b64fhatmepxi1z</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>kllr731wlmbb8njvb1v1bavris</td>\n",
       "      <td>[06758428]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251788</th>\n",
       "      <td>2zre7jvgxbif6cdpqt</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>rmoccnmpqfwrz3jlzhxvt7udkj</td>\n",
       "      <td>[07420209]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251789</th>\n",
       "      <td>34xazujbk01kpe0bgr</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>xzxxkfrcmsyfcqqrcohlba5wkn</td>\n",
       "      <td>[03563623]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251790</th>\n",
       "      <td>34xazujbk01kpe0bgr</td>\n",
       "      <td>skipass</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>xzxxkfrcmsyfcqqrcohlba5wkn</td>\n",
       "      <td>[03563623]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251791</th>\n",
       "      <td>g0tkwbe686jnynbxs9</td>\n",
       "      <td>experience</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>wsunenfdtesnbkfiqjhahhsofp</td>\n",
       "      <td>[01161471335350321318942]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251792</th>\n",
       "      <td>g0tkwbe686jnynbxs9</td>\n",
       "      <td>experience</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>wsunenfdtesnbkfiqjhahhsofp</td>\n",
       "      <td>[01161471335350321318942]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160060 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 order.uid product.type item.snowitcardNumber  \\\n",
       "0       berfovxwyu72mzwmei     giftcard                  <NA>   \n",
       "2       sgo1b64fhatmepxi1z      skipass                  <NA>   \n",
       "3       sgo1b64fhatmepxi1z      skipass                  <NA>   \n",
       "4       sgo1b64fhatmepxi1z      skipass                  <NA>   \n",
       "6       sgo1b64fhatmepxi1z      skipass                  <NA>   \n",
       "...                    ...          ...                   ...   \n",
       "251788  2zre7jvgxbif6cdpqt      skipass                  <NA>   \n",
       "251789  34xazujbk01kpe0bgr      skipass                  <NA>   \n",
       "251790  34xazujbk01kpe0bgr      skipass                  <NA>   \n",
       "251791  g0tkwbe686jnynbxs9   experience                  <NA>   \n",
       "251792  g0tkwbe686jnynbxs9   experience                  <NA>   \n",
       "\n",
       "                          user.uid                   card.uid  \n",
       "0       angticjoeipyve4gpwgwd7nv53                 [07240571]  \n",
       "2       kllr731wlmbb8njvb1v1bavris                 [06758428]  \n",
       "3       kllr731wlmbb8njvb1v1bavris                 [06758428]  \n",
       "4       kllr731wlmbb8njvb1v1bavris                 [06758428]  \n",
       "6       kllr731wlmbb8njvb1v1bavris                 [06758428]  \n",
       "...                            ...                        ...  \n",
       "251788  rmoccnmpqfwrz3jlzhxvt7udkj                 [07420209]  \n",
       "251789  xzxxkfrcmsyfcqqrcohlba5wkn                 [03563623]  \n",
       "251790  xzxxkfrcmsyfcqqrcohlba5wkn                 [03563623]  \n",
       "251791  wsunenfdtesnbkfiqjhahhsofp  [01161471335350321318942]  \n",
       "251792  wsunenfdtesnbkfiqjhahhsofp  [01161471335350321318942]  \n",
       "\n",
       "[160060 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_with_card = pd.merge(\n",
    "    details_orders,\n",
    "    single_card_users,\n",
    "    on=\"user.uid\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "details_with_card[details_with_card['item.snowitcardNumber'].isna() & details_with_card['card.uid'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13f46e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map order.uid -> card.uid \n",
    "order_card_map = (\n",
    "    details_with_card\n",
    "    .dropna(subset=[\"card.uid\"])\n",
    "    .assign(card_uid_str=lambda df: df[\"card.uid\"].astype(str)) \n",
    "    .set_index(\"order.uid\")[\"card_uid_str\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# fill NaN in order_details_df\n",
    "order_details_df[\"item.snowitcardNumber\"] = order_details_df[\"item.snowitcardNumber\"].fillna(\n",
    "    order_details_df[\"order.uid\"].map(order_card_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1870cc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item.uid                       0\n",
       "order.uid                      0\n",
       "item.date                  36264\n",
       "product.uid                    0\n",
       "product.dynamicPricing         0\n",
       "item.amount                    0\n",
       "item.discount                  0\n",
       "product.type                   0\n",
       "item.zoneName                  0\n",
       "product.durationHours     154503\n",
       "item.profiles                  0\n",
       "item.variantName               0\n",
       "item.slotName                  0\n",
       "item.snowitcardNumber     544706\n",
       "item.status                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_details_df.isna().sum()\n",
    "# I'v reduced the null values in the item.snowitcardNumber from 704766 to 544706, the others are set to 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee34e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_df['item.snowitcardNumber'] = order_details_df['item.snowitcardNumber'].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d319d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_df['product.durationHours'] = order_details_df['product.durationHours'].fillna(-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "241fee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item.uid                      0\n",
       "order.uid                     0\n",
       "item.date                 36264\n",
       "product.uid                   0\n",
       "product.dynamicPricing        0\n",
       "item.amount                   0\n",
       "item.discount                 0\n",
       "product.type                  0\n",
       "item.zoneName                 0\n",
       "product.durationHours         0\n",
       "item.profiles                 0\n",
       "item.variantName              0\n",
       "item.slotName                 0\n",
       "item.snowitcardNumber         0\n",
       "item.status                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_details_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60f542",
   "metadata": {},
   "source": [
    "## REVIEWS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbf3f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Reviews =====\n",
      "Shape: (93429, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93429 entries, 0 to 93428\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   review.uid  93429 non-null  string\n",
      " 1   user.uid    93429 non-null  string\n",
      " 2   text        93429 non-null  object\n",
      "dtypes: object(1), string(2)\n",
      "memory usage: 2.1+ MB\n",
      "None\n",
      "Missing values per column:\n",
      "review.uid    0\n",
      "user.uid      0\n",
      "text          0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "basic_info(reviews_df, \"Reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48766a83",
   "metadata": {},
   "source": [
    "## LABELLED REVIEWS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68ea6f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Labelled Reviews =====\n",
      "Shape: (327522, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 327522 entries, 0 to 327521\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   labelled_review.uid  327522 non-null  string\n",
      " 1   text                 327522 non-null  object\n",
      " 2   sentiment_label      327522 non-null  string\n",
      "dtypes: object(1), string(2)\n",
      "memory usage: 7.5+ MB\n",
      "None\n",
      "Missing values per column:\n",
      "labelled_review.uid    0\n",
      "text                   0\n",
      "sentiment_label        0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "basic_info(labelled_reviews_df, \"Labelled Reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886444c",
   "metadata": {},
   "source": [
    "# SAVE CORRECTED DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "058a8098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected users data saved in C:/Users/davyt/Desktop/Bicocca/Marketing Analytics/unimib_snowit_project/data_loaded/users.pkl\n",
      "Corrected profiles data saved in C:/Users/davyt/Desktop/Bicocca/Marketing Analytics/unimib_snowit_project/data_loaded/profiles.pkl\n",
      "Corrected cards data saved in C:/Users/davyt/Desktop/Bicocca/Marketing Analytics/unimib_snowit_project/data_loaded/cards.pkl\n",
      "Corrected orders data saved in C:/Users/davyt/Desktop/Bicocca/Marketing Analytics/unimib_snowit_project/data_loaded/orders.pkl\n",
      "Corrected order details data saved in C:/Users/davyt/Desktop/Bicocca/Marketing Analytics/unimib_snowit_project/data_loaded/order_details.pkl\n",
      "Corrected reviews data saved in C:/Users/davyt/Desktop/Bicocca/Marketing Analytics/unimib_snowit_project/data_loaded/reviews.pkl\n",
      "Corrected laballed reviews data saved in C:/Users/davyt/Desktop/Bicocca/Marketing Analytics/unimib_snowit_project/data_loaded/reviews_labelled.pkl\n"
     ]
    }
   ],
   "source": [
    "users_pkl_path = pkl_paths['users']\n",
    "with users_pkl_path.open('wb') as fh:\n",
    "    dill.dump(users_df, fh)\n",
    "\n",
    "print(f\"Corrected users data saved in {users_pkl_path.as_posix()}\")\n",
    "####\n",
    "\n",
    "profiles_pkl_path = pkl_paths['profiles']\n",
    "with profiles_pkl_path.open('wb') as fh:\n",
    "    dill.dump(profiles_df, fh)\n",
    "\n",
    "print(f\"Corrected profiles data saved in {profiles_pkl_path.as_posix()}\")\n",
    "####\n",
    "\n",
    "cards_pkl_path = pkl_paths['cards']\n",
    "with cards_pkl_path.open('wb') as fh:\n",
    "    dill.dump(cards_df, fh)\n",
    "\n",
    "print(f\"Corrected cards data saved in {cards_pkl_path.as_posix()}\") \n",
    "\n",
    "####\n",
    "orders_pkl_path = pkl_paths['orders']\n",
    "with orders_pkl_path.open('wb') as fh:\n",
    "    dill.dump(orders_df, fh)\n",
    "\n",
    "print(f\"Corrected orders data saved in {orders_pkl_path.as_posix()}\") \n",
    "\n",
    "####\n",
    "order_details_pkl_path = pkl_paths['order_details']\n",
    "with order_details_pkl_path.open('wb') as fh:\n",
    "    dill.dump(order_details_df, fh)\n",
    "\n",
    "print(f\"Corrected order details data saved in {order_details_pkl_path.as_posix()}\") \n",
    "\n",
    "####\n",
    "reviews_pkl_path = pkl_paths['reviews']\n",
    "with reviews_pkl_path.open('wb') as fh:\n",
    "    dill.dump(reviews_df, fh)\n",
    "\n",
    "print(f\"Corrected reviews data saved in {reviews_pkl_path.as_posix()}\") \n",
    "\n",
    "####\n",
    "reviews_labelled_pkl_path = pkl_paths['reviews_labelled']\n",
    "with reviews_labelled_pkl_path.open('wb') as fh:\n",
    "    dill.dump(labelled_reviews_df, fh)\n",
    "\n",
    "print(f\"Corrected laballed reviews data saved in {reviews_labelled_pkl_path.as_posix()}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MarketingAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
